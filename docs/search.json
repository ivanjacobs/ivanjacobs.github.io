[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I have 22 years‚Äô experience as Computer Scientist and Data Scientist 18 of which in the European Institutions. I completed a Master of Data Science at Rochester Institute of Technology (RIT) and have followed multiple professional education paths to acquire the needed knowledge and skills. Since 2008 I have been delivering A.I. consulting all over the world in the areas of Physical Security, Telecom, Communication, Cyber Security, Industry, Robotics, Naval Solutions and Public Sector.\nCurrently following a path towards a PhD at the Swiss School of Business Research.\nüî≠ I‚Äôm currently working on A.I. applied in health and biosciences.\nPublications:\n‚ú® Jacobs, Ivan, and Manolis Maragoudakis. 2021. ‚ÄúDe Novo Drug Design Using Artificial Intelligence Applied on SARS-CoV-2 Viral Proteins ASYNT-GAN‚Äù BioChem 1, no. 1: 36-48. https://doi.org/10.3390/biochem1010004\n‚ú® Preprint:In silico Antibody-Peptide Epitope prediction for Personalized cancer therapy Ivan Jacobs, Lim Chwee Ming, Jamie Mong, Maragkoudakis Emmanoil, Nishant Malik\nbioRxiv 2023.01.23.525181; doi: https://doi.org/10.1101/2023.01.23.525181"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Intractable inference problems\n\n\n\n\n\n\n\nEvidence Lower Bound\n\n\nIntractable Inference\n\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nIvan Jacobs\n\n\n\n\n\n\n  \n\n\n\n\nPublication:In silico Antibody-Peptide Epitope prediction for Personalized cancer therapy\n\n\n\n\n\n\n\nPersonalized therapy\n\n\nnetworks\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nIvan Jacobs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nIvan Jacobs\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/personalized therapy/index.html",
    "href": "posts/personalized therapy/index.html",
    "title": "Publication:In silico Antibody-Peptide Epitope prediction for Personalized cancer therapy",
    "section": "",
    "text": "The human leukocyte antigen (HLA) system is a complex of genes on chromosome 6 in humans that encodes cell-surface proteins responsible for regulating the immune system. Viral peptides presented to cancer cell surfaces by the HLA trigger the immune system to kill the cells, creating Antibody-peptide epitopes (APE). This study proposes an in-silico approach to identify patient-specific APEs by applying complex networks diagnostics on a novel multiplex data structure as input for a deep learning model. The proposed analytical model identifies patient and tumor-specific APEs with as few as 20 labeled data points.\nAdditionally, the proposed data structure employs complex network theory and other statistical approaches that can better explain and reduce the black box effect of deep learning. The proposed approach achieves an F1-score of 80\\% and 93\\% on patients one and two respectively and above 90\\% on tumor-specific tasks. Additionally, it minimizes the required training time and the number of parameters.\nThe human leukocyte antigen (HLA) system or complex is a complex of genes on chromosome 6 in humans that encode cell-surface proteins responsible for regulating the immune system. The HLA system also known as the human version of the major histocompatibility complex (MHC) is found in many animals.\nHLA genes are highly polymorphic, which means that there are thousands of different forms of these genes called alleles, allowing them to fine-tune the adaptive immune system. The proteins encoded by certain genes are also known as antigens, because of their historic discovery as factors in organ transplants.\nAs shown in Figure \\ref{fig:1} HLA‚Äôs proteins present viral peptides from inside the cell to the surface of the cell. For example, if the cell is infected by a virus or is cancerous, the HLA system brings abnormal fragments, called peptides, to the surface of the cell so that the cell can be destroyed by the immune system.\n\n\n\nHLA proteins (green) display peptides (red) from inside the cell to help immune cells find cancerous or infected cells\n\n\nPredicting the specific HLA peptide combination that will present the peptide to the cell‚Äôs surface permits the creation of a treatment that will trigger the human immune system to destroy the cell.\nSpecifically, in cancer, this ability is essential, given that cancer is highly mutagenic with tumor and patient-specific mutations. This means that patients with the same tumor type will have different mutations that result in different reactions to the same treatment.\nAdvances in Deoxyribonucleic acid (DNA) sequencing, Messenger Ribonucleic acid (mRNA) vaccines, and high computational power allow us to work toward patient-specific therapy. This approach, called personalized mRNA-based antitumor vaccine, visualized in Figure \\ref{fig:2}, is bound to play a major role in the future.\n\n\n\nThe exome of tumor cells isolated from a biopsy sample and the exome of normal cells are compared to identify tumor-specific mutations. Point non-synonymous mutations, gene deletions, or rearrangements can give rise to neoantigens. Several bioinformatics tools are used to predict major histocompatibility complex (MHC) class I and class II binding (necessary for recognition by T cells) and RNA expression presence of the mutated antigen among tumor cells (clonality). RNA sequencing enables verification that the gene encoding the neoantigen is actually expressed by tumor cells. A tandem gene encoding several neoantigen peptides is cloned into a plasmid and transcribed to mRNA. Finally, these mRNAs are injected as naked RNA, formulated into liposomes, or loaded into dendritic cells.\n\n\nThe approach is meant to trigger an antitumor immune response in patients by challenging them with mRNAs encoding tumor-specific antigens \\citep{pastor_rna_2018}. These mRNAs can be directly injected as naked RNA or loaded into patient-derived dendritic cells.\nIn this work, we propose to extend the approach with additional laboratory and analytical optimization steps.\nConcretely DNA sequenced from the patient is used to select candidate peptides that will result from gene expression. As the space of possible combinations is huge a subset of potential peptides is synthesized and their reaction to the patient‚Äôs specific HLA alleles is tested by applying an enzyme-linked immunospot (ELISpot) assay.\nAn enzyme-linked immunospot (ELISpot) assay \\citep{engvall_enzyme-linked_1971}, shown in Figure \\ref{fig:3}, is a highly versatile and sensitive technique that is used for qualitative and quantitative measurement of the cytokine-secreting cells at the single-cell level. \\citep{paulie_chapter_2006}\n\n\n\nVisualisation of an enzyme-linked immunospot (ELISpot) Assa"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hi, this is Ivan. I‚Äôm documenting my research journey here"
  },
  {
    "objectID": "posts/Intractable inference problems/index.html",
    "href": "posts/Intractable inference problems/index.html",
    "title": "Intractable inference problems",
    "section": "",
    "text": "Sources: Matthew N. Bernstein blog, Deep Learning Book, Ian Good Fellow\nThe term inference usually refers to computing the probability distribution of a set of variable when provided another set of variables. We usually are interested in computing \\(p(z|v)\\), in a context of latent variable or multitask learning when the task is defined by a vector \\(z\\). The challenge lays in the difficulty of computing \\(p(z|v)\\) or the expected value with respect to it \\(E_{z‚àºp(z|v)}\\)\nIntractable inference in deep learning arises from the connections between the activations generated from the multitude of layers that create either mutual ancestors or large cliques of activations. In order to solve the intractable inference problem we may approach it as an optimization problem and derive approximate inference algorithms that will approximate the underlying exact inference optimization.\nTo define an optimization problem we can assume that we have an observed data \\(v\\) that is a realization of a random variable \\(V\\). We put forward the existence of another random variable \\(Z\\) and \\(V\\) and \\(Z\\) are distributed according a joint distribution \\(p(X,Z;\\theta)\\) where \\(\\theta\\) parameterizes the distribution. The joint distribution indicates that \\(Z\\) and \\(V\\) are strongly correlated. Suppose \\(m_z,m_v\\) are the means of \\(z\\) and \\(v\\) and are known, then we might be interested in the \\(\\sigma_{zv}\\) the covariance that measures the connection of \\(z\\) and \\(v\\). \\[\\sigma_{zv}=E[(z-m_z)(v-m_v)] \\tag{1}\\]\nHence, to compute \\(\\sigma_{zv}\\) it is not enough to know the probability of \\(z\\) and the probability of each \\(v\\) but the joint probability of each pair \\(z\\) and \\(v\\).\nSuppose we run separate 2 experiments, the covariance between experiments for all pairs of \\(z_i\\) and \\(v_j\\) then equation (Equation¬†1)looks like: \\[\n\\begin{aligned}\n\\sigma_{12}=E([(z-m_1)(v-m_2)])\n\\\\\n\\sigma_{12}=\\sum_{all}\\sum_{ij}p_{ij}(z_i-m_1)(y_j-m_2)\n\\end{aligned}\n\\tag{2}\\] But \\(v\\) is a realization from \\(V\\) and not from \\(Z\\) and therefor \\(z\\) remains ‚Äúlatent‚Äù i.e.¬†not observed. We might be interested in either computing the posteriour distribution \\(p(Z|V;\\theta)\\) given a fixed \\(\\theta\\) or finding the maximum likelihood \\(argmaxv_{\\theta}l(\\theta)\\) where \\(l(\\theta)\\) is the log-likelihood function given an unknown \\(\\theta\\) defined by: \\[\nl(\\theta):=\\log p(v;\\theta)=\\log\\int_{z}p(v,z;\\theta)dz\n\\tag{3}\\] We could envisage to compute the log-probability of the observed variable \\(v\\) \\(\\log p(v;\\theta)\\) but sometimes it is is to costly to marginalize out \\(h\\) and this computation becomes difficult. Instead we can compute the evidence lower bound (ELBO) or variatonal free energy \\(\\mathcal{L}(v,\\theta,q)\\) on \\(\\log(v;\\theta)\\). The evidence, in evidence lower bound, is the likelihood evaluated at a fixed \\(\\theta\\) : \\[\nevidence:\\log p(v;\\theta)\n\\tag{4}\\]\nHence, if we have approximated \\(\\theta\\) well enough through our optimization approach we would expect that the marginal probability of the observed variable \\(v\\) will be high. The evidence thus quantifies the quality of the approximation of \\(p_{model}\\) parameterized by \\(\\theta\\) of \\(p_{data}\\) i.e.¬†\\(p_{model}(z|v;\\theta)\\approx p_{data}(z|v)\\).\nIf we consider that \\(Z\\) follows an arbitrary probability distribution \\(q\\) and that the joint distribution \\(p(v,z;\\theta):=p(v|z;\\theta)q(z)\\), then the evidence lower bound is the lower bound on the evidence that makes use of \\(q\\). Concretely : \\[\n\\begin{aligned}\n\\log p(v;\\theta)\\ge \\mathbb{E}_{z\\sim q}\\left[ \\log \\frac{p(v;z;\\theta)}{q(z)} \\right]\n\\end{aligned}\n\\tag{5}\\] where ELBO is the right-hand side of Equation¬†5\n\\[\n\\begin{aligned}\nELBO:= \\mathbb{E}_{z\\sim q}\\left[ \\log \\frac{p(v;z;\\theta)}{q(z)} \\right]\n\\end{aligned}\n\\tag{6}\\]\nThe gap between the the evidence and the ELBO is the Kullback-Leibler divergence between \\(p(z|v;\\theta)\\). \\[\n\\begin{aligned}\nD_{KL}(q(z)|| p(z|v;\\theta))\n\\end{aligned}\n\\tag{7}\\] This lays the basis of the approximation approach called variational inference where we learn to infer \\(q\\) through optimization algorithm. As long as \\(z\\) is continuous, we can back-propagate through samples of \\(z\\) dawn from \\(q(z|v)=q(z;f(v;\\theta))\\) to obtain a gradient with respect to \\(\\theta\\) in order to maximize \\(\\mathcal{L}(v,\\theta,q)\\). We can write \\(\\mathcal{L}(v,\\theta,q)\\) as:\n\\[\n\\begin{aligned}\n\\mathcal{L}(v,\\theta,q)=\\log p(v;\\theta) - D_{KL}(q(z|v)||p(z|v;\\theta))\n\\end{aligned}\n\\tag{8}\\]\nwhere \\(q\\) is an arbitrary probability distribution over \\(z\\). The difference between the expectation \\(\\log p(v)\\) and \\(\\mathcal{L}(v,\\theta,q)\\) is given by the KL divergence that is always positive. We can conclude that \\(\\mathcal{L}\\) has at most the same value as the desired log-probability. If the two are equal \\(q\\) has the same distribution as \\(p(z|v)\\)\nWe can rearrange \\(\\mathcal{L}\\) into:\n\\[\n\\begin{aligned}\n\\mathcal{L}(v,\\theta,q)=\\log p(v;\\theta) - D_{KL}(q(z|v)||p(z|v;\\theta))\n\\\\\n=\\log p(v;\\theta) - \\mathbb{E}_{z\\sim q}\\log \\frac{q(z|v)}{p(z|v)}\n\\\\\n=\\log p(v;\\theta) - \\mathbb{E}_{z\\sim q}\\log \\frac{q(z|v)}{\\frac{p(z,v;\\theta)}{p(v;\\theta)}}\n\\\\\n=\\log p(v;\\theta) - \\mathbb{E}_{z\\sim q} \\left[\\log q(z|v)-\\log p(z,v;\\theta) + \\log p(v;\\theta) \\right]\n\\\\\n=-\\mathbb{E}_{z\\sim q} \\left[\\log q(z|v) - \\log p(z,v;\\theta) \\right]\n\\end{aligned} \\tag{9}\\]\nHence a more canonical definition of the evidence lower bound can be defined as: \\[\n\\begin{aligned}\n\\mathcal{L}(v,\\theta,q)=\\mathbb{E}_{z\\sim q} \\left[\\log p(z,v)\\right]+ H(q)\n\\end{aligned} \\tag{10}\\]\nFor an appropriate choice of \\(q,\\mathcal{L}\\) is tractable. For \\(q(z|v)\\) that approximates \\(p(z|v)\\) better, the lower bound \\(\\mathcal{L}\\) is closer to \\(\\log p(v)\\). When \\(q(z|v)=p(z|v)\\), the approximation is perfect, hence, \\(\\mathcal{L}=\\log p(v;\\theta)\\). Thus, we can think of variational inference as the procedure of finding \\(q\\) that maximizes \\(\\mathcal{L}\\)"
  }
]